{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f17ea7f-20ab-48a4-aa95-9ab5205015eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/home/kaor/.conda/envs/mmdet/bin/python -m pip install -U protobuf==3.20.0\n",
    "# !/home/kaor/.conda/envs/mmdet/bin/python -m pip install -U iterative-stratification==0.1.7\n",
    "# !/home/kaor/.conda/envs/mmdet/bin/python -m pip install -U transformers==4.21.2\n",
    "# !/home/kaor/.conda/envs/mmdet/bin/python -m pip install -U tokenizers==0.12.1\n",
    "#!export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be25d735-5218-44cf-86fc-8234546973be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from albumentations.core.transforms_interface import DualTransform, BasicTransform\n",
    "class NLPTransform(BasicTransform):\n",
    "    LANGS = {\n",
    "        'en': 'english',\n",
    "    }\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return {\"data\": self.apply}\n",
    "    \n",
    "    def update_params(self, params, **kwargs):\n",
    "        if hasattr(self, \"interpolation\"):\n",
    "            params[\"interpolation\"] = self.interpolation\n",
    "        if hasattr(self, \"fill_value\"):\n",
    "            params[\"fill_value\"] = self.fill_value\n",
    "        return params\n",
    "\n",
    "    def get_sentences(self, text, lang='en'):\n",
    "        return sent_tokenize(text, self.LANGS.get(lang, 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68eabc7-7db3-43b9-a321-bdc5f480702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleSentencesTransform(NLPTransform):\n",
    "    \"\"\" Do shuffle by sentence \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = self.get_sentences(text, lang)\n",
    "        random.shuffle(sentences)\n",
    "        return ' '.join(sentences), lang\n",
    "\n",
    "class ExcludeDuplicateSentencesTransform(NLPTransform):\n",
    "    \"\"\" Exclude equal sentences \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeDuplicateSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = []\n",
    "        for sentence in self.get_sentences(text, lang):\n",
    "            sentence = sentence.strip()\n",
    "            if sentence not in sentences:\n",
    "                sentences.append(sentence)\n",
    "        return ' '.join(sentences), lang\n",
    "    \n",
    "class ExcludeNumbersTransform(NLPTransform):\n",
    "    \"\"\" exclude any numbers \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'[0-9]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text, lang \n",
    "class ExcludeHashtagsTransform(NLPTransform):\n",
    "    \"\"\" Exclude any hashtags with # \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text, lang\n",
    "class ExcludeUsersMentionedTransform(NLPTransform):\n",
    "    \"\"\" Exclude @users \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text, lang\n",
    "class ExcludeUrlsTransform(NLPTransform):\n",
    "    \"\"\" Exclude urls \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'https?\\S+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text, lang\n",
    "class SwapWordsTransform(NLPTransform):\n",
    "    \"\"\" Swap words next to each other \"\"\"\n",
    "    def __init__(self, swap_distance=1, swap_probability=0.1, always_apply=False, p=0.5):\n",
    "        \"\"\"  \n",
    "        swap_distance - distance for swapping words\n",
    "        swap_probability - probability of swapping for one word\n",
    "        \"\"\"\n",
    "        super(SwapWordsTransform, self).__init__(always_apply, p)\n",
    "        self.swap_distance = swap_distance\n",
    "        self.swap_probability = swap_probability\n",
    "        self.swap_range_list = list(range(1, swap_distance+1))\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        words = text.split()\n",
    "        words_count = len(words)\n",
    "        if words_count <= 1:\n",
    "            return text, lang\n",
    "\n",
    "        new_words = {}\n",
    "        for i in range(words_count):\n",
    "            if random.random() > self.swap_probability:\n",
    "                new_words[i] = words[i]\n",
    "                continue\n",
    "    \n",
    "            if i < self.swap_distance:\n",
    "                new_words[i] = words[i]\n",
    "                continue\n",
    "    \n",
    "            swap_idx = i - random.choice(self.swap_range_list)\n",
    "            new_words[i] = new_words[swap_idx]\n",
    "            new_words[swap_idx] = words[i]\n",
    "\n",
    "        return ' '.join([v for k, v in sorted(new_words.items(), key=lambda x: x[0])]), lang\n",
    "class CutOutWordsTransform(NLPTransform):\n",
    "    \"\"\" Remove random words \"\"\"\n",
    "    def __init__(self, cutout_probability=0.05, always_apply=False, p=0.5):\n",
    "        super(CutOutWordsTransform, self).__init__(always_apply, p)\n",
    "        self.cutout_probability = cutout_probability\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        words = text.split()\n",
    "        words_count = len(words)\n",
    "        if words_count <= 1:\n",
    "            return text, lang\n",
    "        \n",
    "        new_words = []\n",
    "        for i in range(words_count):\n",
    "            if random.random() < self.cutout_probability:\n",
    "                continue\n",
    "            new_words.append(words[i])\n",
    "\n",
    "        if len(new_words) == 0:\n",
    "            return words[random.randint(0, words_count-1)], lang\n",
    "\n",
    "        return ' '.join(new_words), lang\n",
    "class AddNonToxicSentencesTransform(NLPTransform):\n",
    "    \"\"\" Add random non toxic statement \"\"\"\n",
    "    def __init__(self, non_toxic_sentences, sentence_range=(1, 3), always_apply=False, p=0.5):\n",
    "        super(AddNonToxicSentencesTransform, self).__init__(always_apply, p)\n",
    "        self.sentence_range = sentence_range\n",
    "        self.non_toxic_sentences = non_toxic_sentences\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "\n",
    "        sentences = self.get_sentences(text, lang)\n",
    "        for i in range(random.randint(*self.sentence_range)):\n",
    "            sentences.append(random.choice(self.non_toxic_sentences))\n",
    "        \n",
    "        random.shuffle(sentences)\n",
    "        return ' '.join(sentences), lang \n",
    "    \n",
    "import albumentations\n",
    "def get_train_transforms():\n",
    "    return albumentations.Compose([\n",
    "        ExcludeDuplicateSentencesTransform(p=0.9),\n",
    "        ShuffleSentencesTransform(p=0.8),\n",
    "        ExcludeNumbersTransform(p=0.8),\n",
    "        ExcludeHashtagsTransform(p=0.5),\n",
    "        ExcludeUsersMentionedTransform(p=0.9),\n",
    "        ExcludeUrlsTransform(p=0.9),\n",
    "        CutOutWordsTransform(p=0.1),\n",
    "        SwapWordsTransform(p=0.1),\n",
    "    ])\n",
    "train_transforms = get_train_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c497fb-e6ee-4286-a203-f59c07d11c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ac6ea1-a600-4309-8e56-ab2ef0118f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>datasets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sorry, for the language, but Iâm at this poi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Main Author of Wikipedia's Steele Dossier ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Put this pussy right in your face\\nSwipe your ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You donât watch foxâs news because that as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>America is tired o self absorbed divisive scum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19445</th>\n",
       "      <td>middle east\\n- middle east\\n- middle east\\n- ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19446</th>\n",
       "      <td>those with mental disorders are a heavy finan...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19447</th>\n",
       "      <td>koreans are too loud and menacing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19448</th>\n",
       "      <td>when i was a kid, before i knew anything about...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19449</th>\n",
       "      <td>we need unity without homogenization......\\ t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19450 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  datasets\n",
       "0      Sorry, for the language, but Iâm at this poi...         0\n",
       "1      The Main Author of Wikipedia's Steele Dossier ...         0\n",
       "2      Put this pussy right in your face\\nSwipe your ...         0\n",
       "3      You donât watch foxâs news because that as...         0\n",
       "4      America is tired o self absorbed divisive scum...         0\n",
       "...                                                  ...       ...\n",
       "19445   middle east\\n- middle east\\n- middle east\\n- ...         2\n",
       "19446   those with mental disorders are a heavy finan...         2\n",
       "19447                  koreans are too loud and menacing         2\n",
       "19448  when i was a kid, before i knew anything about...         2\n",
       "19449   we need unity without homogenization......\\ t...         2\n",
       "\n",
       "[19450 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "parler = pd.read_csv(\"parler-hate-speech/parler_annotated_data.csv\")\n",
    "parler['datasets'] = 0\n",
    "implicit = pd.read_csv(\"parler-hate-speech/implicit_hate_multi7.csv\")\n",
    "implicit['datasets'] = 1\n",
    "toxigen = pd.read_parquet(\"parler-hate-speech/toxigen.parquet\")\n",
    "toxigen['datasets'] = 2\n",
    "toxigen['text'] = toxigen['generation']\n",
    "datasets = pd.concat([parler.sample(frac=0.65),implicit,toxigen.sample(frac=0.026)])[['text','datasets']].reset_index(drop=True)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08fc1a49-933d-4329-b85e-d1ec983bf1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6579\n",
       "2    6525\n",
       "1    6346\n",
       "Name: datasets, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['datasets'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5fce849-8ad0-4caf-86d0-6d8dab36d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=False\n",
    "    competition='FB3'\n",
    "    _wandb_kernel='nakama'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    print_freq=100\n",
    "    num_workers=6\n",
    "    model=\"microsoft/deberta-v3-base\"#\"Narrativaai/deberta-v3-small-finetuned-hate_speech18\"#\"microsoft/deberta-v3-base\"\n",
    "    gradient_checkpointing=True\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=4\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=16\n",
    "    max_len=640\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    target_cols=['datasets']\n",
    "    seed=42\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]\n",
    "    train=True\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e78d6190-5d92-4b86-b7f3-dc4194f6757f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.12.1\n",
      "transformers.__version__: 4.21.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "# os.system('pip install iterative-stratification==0.1.7')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# os.system('pip uninstall -y transformers')\n",
    "# os.system('pip uninstall -y tokenizers')\n",
    "# os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels transformers')\n",
    "# os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels tokenizers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c8f010-257a-4347-aecb-cdbe4a818a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"outputs_adv/\" + CFG.model.replace(\"/\",\".\")+\"/\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2d4652-00ee-4ace-a13e-f25b25790699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "from sklearn.metrics import accuracy_score\n",
    "def get_score(y_trues, y_preds):\n",
    "    scores = accuracy_score(y_trues, y_preds.argmax(1))\n",
    "    return scores , scores\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+\"/train\"):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger(filename=OUTPUT_DIR+\"/train\")\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f391d4c-ccb4-4ccd-9002-c6116739eb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (19450, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>datasets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sorry, for the language, but Iâm at this poi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Main Author of Wikipedia's Steele Dossier ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Put this pussy right in your face\\nSwipe your ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You donât watch foxâs news because that as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>America is tired o self absorbed divisive scum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  datasets\n",
       "0  Sorry, for the language, but Iâm at this poi...         0\n",
       "1  The Main Author of Wikipedia's Steele Dossier ...         0\n",
       "2  Put this pussy right in your face\\nSwipe your ...         0\n",
       "3  You donât watch foxâs news because that as...         0\n",
       "4  America is tired o self absorbed divisive scum...         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape: (19450, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>datasets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sorry, for the language, but Iâm at this poi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Main Author of Wikipedia's Steele Dossier ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Put this pussy right in your face\\nSwipe your ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You donât watch foxâs news because that as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>America is tired o self absorbed divisive scum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  datasets\n",
       "0  Sorry, for the language, but Iâm at this poi...         0\n",
       "1  The Main Author of Wikipedia's Steele Dossier ...         0\n",
       "2  Put this pussy right in your face\\nSwipe your ...         0\n",
       "3  You donât watch foxâs news because that as...         0\n",
       "4  America is tired o self absorbed divisive scum...         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.shape: (19450, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>datasets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sorry, for the language, but Iâm at this poi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Main Author of Wikipedia's Steele Dossier ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Put this pussy right in your face\\nSwipe your ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You donât watch foxâs news because that as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>America is tired o self absorbed divisive scum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  datasets\n",
       "0  Sorry, for the language, but Iâm at this poi...         0\n",
       "1  The Main Author of Wikipedia's Steele Dossier ...         0\n",
       "2  Put this pussy right in your face\\nSwipe your ...         0\n",
       "3  You donât watch foxâs news because that as...         0\n",
       "4  America is tired o self absorbed divisive scum...         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = datasets\n",
    "test = datasets\n",
    "submission = datasets\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "display(train.head())\n",
    "print(f\"test.shape: {test.shape}\")\n",
    "display(test.head())\n",
    "print(f\"submission.shape: {submission.shape}\")\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc5e01d0-4b5e-4199-a52a-dd1cee5d08e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    4863\n",
       "1    4863\n",
       "2    4862\n",
       "3    4862\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ac71027-ba52-4cbf-b99f-57ab80c59d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e805980-dc51-4b98-b208-9f84655a9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "714c2747-5a0e-422a-b8fb-e51f056b225b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a6a6b9237c463cab4bc7780d7a93b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_len: 465\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "lengths = []\n",
    "tk0 = tqdm(train['text'].fillna(\"\").values, total=len(train))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "CFG.max_len = max(lengths) + 3 # cls & sep & sep\n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b78b9ac2-13f6-466d-bfda-6b919d81bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df , train_transforms = None):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "        self.train_transforms = None\n",
    "        self.lang = 'en'\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text  = self.texts[item]\n",
    "        if self.train_transforms:\n",
    "            text, _ = self.train_transforms(data=(text, self.lang))['data']\n",
    "        inputs = prepare_input(self.cfg, text)\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.long)\n",
    "        return inputs, label\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff900d1f-607b-4d82-904c-a0a591a4feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "            LOGGER.info(self.config)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27ef46f2-61a5-4716-94ad-d6d36940c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels[:,0])\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels[:,0])\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02a4a33f-72d2-434f-9b83-8c9cdc037849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds , train_transforms = train_transforms)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds , train_transforms = None)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.CrossEntropyLoss() #nn.SmoothL1Loss(reduction='mean') # RMSELoss(reduction=\"mean\")\n",
    "    \n",
    "    best_score = 0\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        # scoring\n",
    "        score, scores = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f10c0-c3ac-4e93-9063-fa56718ebd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/911] Elapsed 0m 0s (remain 9m 35s) Loss: 1.2104(1.2104) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][100/911] Elapsed 0m 26s (remain 3m 32s) Loss: 0.2016(0.5327) Grad: 47624.6484  LR: 0.00001996  \n",
      "Epoch: [1][200/911] Elapsed 0m 47s (remain 2m 47s) Loss: 0.3028(0.3927) Grad: 97296.1797  LR: 0.00001985  \n",
      "Epoch: [1][300/911] Elapsed 1m 3s (remain 2m 9s) Loss: 0.0927(0.3431) Grad: 27887.8672  LR: 0.00001967  \n",
      "Epoch: [1][400/911] Elapsed 1m 20s (remain 1m 42s) Loss: 0.0660(0.3119) Grad: 45972.3164  LR: 0.00001941  \n",
      "Epoch: [1][500/911] Elapsed 1m 36s (remain 1m 19s) Loss: 0.1339(0.2869) Grad: 60264.2383  LR: 0.00001908  \n",
      "Epoch: [1][600/911] Elapsed 1m 53s (remain 0m 58s) Loss: 0.0087(0.2713) Grad: 12883.7988  LR: 0.00001869  \n",
      "Epoch: [1][700/911] Elapsed 2m 10s (remain 0m 38s) Loss: 0.1038(0.2589) Grad: 41666.9023  LR: 0.00001823  \n",
      "Epoch: [1][800/911] Elapsed 2m 26s (remain 0m 20s) Loss: 0.0857(0.2507) Grad: 66970.8594  LR: 0.00001771  \n",
      "Epoch: [1][900/911] Elapsed 2m 42s (remain 0m 1s) Loss: 0.0295(0.2457) Grad: 29314.2109  LR: 0.00001713  \n",
      "Epoch: [1][910/911] Elapsed 2m 44s (remain 0m 0s) Loss: 0.2623(0.2451) Grad: 99602.1797  LR: 0.00001707  \n",
      "EVAL: [0/152] Elapsed 0m 0s (remain 1m 23s) Loss: 0.0029(0.0029) \n",
      "EVAL: [100/152] Elapsed 0m 7s (remain 0m 3s) Loss: 1.7974(0.2252) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2451  avg_val_loss: 0.2124  time: 173s\n",
      "Epoch 1 - Score: 0.9437  Scores: 0.9436561793131811\n",
      "Epoch 1 - Save Best Score: 0.9437 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [151/152] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0064(0.2124) \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df[CFG.target_cols].values\n",
    "        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "        score, scores = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc0fa1-1d31-4b68-b124-0ff62e3d7800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6e887-bf01-4c34-b76b-8b4aa81f3dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mmdet)",
   "language": "python",
   "name": "mmdet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
