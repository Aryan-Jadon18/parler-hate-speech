{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b998b1af-6592-4d53-9778-732566947691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "from model import CustomModel,MeanPooling\n",
    "from model import CFG_base as CFG\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d6bcf8-da87-48ad-a5e2-801196bff6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"OrK7/parler_hate_speech\"\n",
    "downloaded_model_path = hf_hub_download(repo_id=name, filename=\"pytorch_model.bin\")\n",
    "model = torch.load(downloaded_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c317183c-8e53-465f-b45f-f07f1724b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(text):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=512,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(np.array(v).reshape(1,-1), dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d423b13-f50e-43f6-b495-817a1eb6326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "class HatePipeline(Pipeline):\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        preprocess_kwargs = {}\n",
    "        if \"maybe_arg\" in kwargs:\n",
    "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        out = prepare_input(inputs)\n",
    "        return collate(out)\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        outputs = self.model(model_inputs)\n",
    "        return outputs\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        return np.array(model_outputs[0,0].numpy()).clip(0,1)*4+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d13498-3161-4108-9da2-c09c8ecc0afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaor/.conda/envs/mmdet/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.155198812484741"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = HatePipeline(model=model)\n",
    "pipe(\"I Hate #$%#$%Jewish%$#@%^^@#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "912af70f-2c6a-4341-bb5f-53f50a376047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"I Love you #\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "047a3ac5-575f-41a2-b432-1742692b884e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.473475933074951"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"It should be easy to do............itâs just a big mouth and two tits!!!! Wendy Williams is getting a Madame Tussauds wax figure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca5340f-5262-4225-82b2-412914ac9e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5166422724723816"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Report: Google A.I. Is ‘Biased’ Against Gay People, Jews  http://www.breitbart.com/tech/2017/10/26/report-go...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc37fe2-a6b7-413b-9684-2e31297d58be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.308754205703735"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"fuck Gay People, Jews  http://www.breitbart.com/tech/2017/10/26/report-go...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1bef8-8a5a-47ee-85b7-e4d0cac443ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mmdet)",
   "language": "python",
   "name": "mmdet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
