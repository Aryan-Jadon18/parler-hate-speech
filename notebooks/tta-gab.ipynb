{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fde0fa1-1255-48dd-bd3b-eb03e12be30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.12.1\n",
      "transformers.__version__: 4.21.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "# %env TOKENIZERS_PARALLELISM=true\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import argparse\n",
    "from utils import *\n",
    "\n",
    "def get_result(oof_df,is_cla=False):\n",
    "    labels = oof_df[CFG.target_cols].values\n",
    "    preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "    if is_cla:\n",
    "        score, scores = get_score_cla(labels, preds)\n",
    "    else:\n",
    "        score, scores = get_score(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "\n",
    "class CFG:\n",
    "    wandb=False\n",
    "    competition='Hate'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    print_freq=100\n",
    "    num_workers=6\n",
    "    model=\"microsoft/deberta-v3-large\"#\"Narrativaai/deberta-v3-small-finetuned-hate_speech18\"#\"microsoft/deberta-v3-base\"\n",
    "    gradient_checkpointing=True\n",
    "    scheduler='cosine'\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=5\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=12\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    target_cols=['label_mean','HD', 'CV', 'VO','REL', 'RAE', 'SXO', 'GEN', 'IDL', 'NAT', 'POL', 'MPH', 'EX', 'IM']\n",
    "    seed=42\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]\n",
    "    train=True\n",
    "    pretrain_hate = False\n",
    "    back_translation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8cbf55d-eb5b-46ee-95b3-8e6841eb74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./parler-hate-speech/GabHateCorpus_annotations_bt.csv')\n",
    "train[CFG.target_cols] = train[CFG.target_cols] - train[CFG.target_cols].min()\n",
    "train[CFG.target_cols] = train[CFG.target_cols] / train[CFG.target_cols].max()\n",
    "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols+['id']])):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce099c60-05cc-4be5-8d1f-9384ae5794b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c514e42-1484-47cf-8d6c-7461cb48495f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70306eedf5c427ca009d58e1a8a0004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = []\n",
    "tk0 = tqdm(train['text'].fillna(\"\").values, total=len(train))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "CFG.max_len = min(509,max(lengths)) + 3 # cls & sep & sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202ffd80-cc02-42c7-8216-c3fefd5be9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel(CFG, config_path=None, pretrained=True,LOGGER = get_logger(filename=\"temp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53eba2bb-8224-4df8-9e74-ef635771ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df,text_col = \"text\"):\n",
    "        self.cfg = cfg\n",
    "        df = df.fillna(\"-9999\")\n",
    "        self.df = df\n",
    "        self.texts = df[text_col].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        texts = self.texts[item]\n",
    "        inputs = prepare_input(self.cfg, texts)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96354889-2576-45a9-a4ea-192e5d52a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_en = TestDataset(CFG,train,text_col = \"text\")\n",
    "test_loader_en = DataLoader(test_dataset_en,\n",
    "                        batch_size=CFG.batch_size * 2,\n",
    "                        shuffle=False,\n",
    "                        num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "test_dataset_de = TestDataset(CFG,train,text_col = \"BackTranslation_de\")\n",
    "test_loader_de = DataLoader(test_dataset_de,\n",
    "                        batch_size=CFG.batch_size * 2,\n",
    "                        shuffle=False,\n",
    "                        num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "test_dataset_fr = TestDataset(CFG,train,text_col = \"BackTranslation_fr\")\n",
    "test_loader_fr = DataLoader(test_dataset_fr,\n",
    "                        batch_size=CFG.batch_size * 2,\n",
    "                        shuffle=False,\n",
    "                        num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "test_dataset_es = TestDataset(CFG,train,text_col = \"BackTranslation_es\")\n",
    "test_loader_es = DataLoader(test_dataset_es,\n",
    "                        batch_size=CFG.batch_size * 2,\n",
    "                        shuffle=False,\n",
    "                        num_workers=CFG.num_workers, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec97935-b69a-4ef4-bff6-e20503ada739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165fe4cefd4f43d7a9cac72c61e6ec11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1620a5c87e76430887a8757b6235d17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2f273cb38a460eb9d78010061ca04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06456d0e4a854bfa91bcc8fd79f3faa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score,recall_score,roc_auc_score,accuracy_score\n",
    "import glob\n",
    "TH_FROM_BASE = 0.3\n",
    "W = 0.75\n",
    "MODEL = 'microsoft.deberta-v3-large'\n",
    "EXP = 'Gab_backtranslate_cla'\n",
    "CFG.model = 'microsoft/deberta-v3-large'\n",
    "model = CustomModel(CFG, config_path=None, pretrained=True,LOGGER = get_logger(filename=\"temp\"))\n",
    "all_df = []\n",
    "for fold in tqdm([0,1,2,3]):\n",
    "    state = torch.load(glob.glob(f\"./{EXP}/{MODEL}/*{fold}_best.pth\")[0])\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    results_score = []\n",
    "    results_emb = []\n",
    "    for loader in tqdm([test_loader_en,test_loader_de,test_loader_fr,test_loader_es]):\n",
    "        lg_embeddings = []\n",
    "        lg_score = []\n",
    "        for step, (inputs) in enumerate(loader):\n",
    "            inputs = collate(inputs)\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(device)\n",
    "            with torch.no_grad():\n",
    "                sentence_embeddings = model.feature(inputs)\n",
    "                lg_embeddings.append(sentence_embeddings.to('cpu').numpy())\n",
    "                y_preds = model(inputs)\n",
    "                lg_score.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        lg_score = np.concatenate(lg_score)\n",
    "        lg_embeddings = np.concatenate(lg_embeddings)\n",
    "        results_score.append(lg_score)\n",
    "        results_emb.append(lg_embeddings)\n",
    "    results_score = np.array(results_score)\n",
    "    RES = []\n",
    "    for t in range(results_score.shape[2]):\n",
    "        results = []\n",
    "        for i in range(len(results_score[0])):\n",
    "            base = results_score[0][i][t]\n",
    "            l = []\n",
    "            for row in range(1,4):\n",
    "                if abs(results_score[row][i][t] - base) < TH_FROM_BASE:\n",
    "                    l.append(results_score[row][i][t])\n",
    "            if len(l) ==0:\n",
    "                results.append(base)\n",
    "            else:\n",
    "                results.append(W*base+(1-W)*np.mean(l))\n",
    "        RES.append(results)\n",
    "    df_res = pd.DataFrame(RES).T\n",
    "    df_res.columns = [\"pred_\"+row for row in CFG.target_cols]\n",
    "    T = pd.concat([train,df_res],axis=1)\n",
    "    val = T[T['fold']==fold]\n",
    "    all_df.append(val)\n",
    "results_tta = pd.concat(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c72179-ce3b-4211-8b9c-eb9e2092adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e2f92-98b8-4a76-aaf2-c78fd47adb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tta.to_pickle(f\"./{EXP}/{MODEL}/tta.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282a6b0-373d-4ede-9921-1a162668c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True: \n",
    "#     results_score = np.array(results_score)\n",
    "#     RES = []\n",
    "#     for t in range(results_score.shape[2]):\n",
    "#         results = []\n",
    "#         for i in range(len(results_score[0])):\n",
    "#             base = results_score[0][i][t]\n",
    "#             l = []\n",
    "#             for row in range(1,4):\n",
    "#                 if abs(results_score[row][i][t] - base) < TH_FROM_BASE:\n",
    "#                     l.append(results_score[row][i][t])\n",
    "#             if len(l) ==0:\n",
    "#                 results.append(base)\n",
    "#             else:\n",
    "#                 results.append(W*base+(1-W)*np.mean(l))\n",
    "#         RES.append(results)\n",
    "# df_res = pd.DataFrame(RES).T\n",
    "# df_res.columns = [\"pred_\"+row for row in CFG.target_cols]\n",
    "# T = pd.concat([train,df_res],axis=1)\n",
    "# val = T[T['fold']==fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "95f79374-d3ca-4d08-a571-5ce8047a2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_res = []\n",
    "label = []\n",
    "for fold in [0,1,2,3]:\n",
    "    val = train[train['fold']==fold]\n",
    "    s = list(np.array(val[f'tta_score_fold_{fold}']).reshape(-1))\n",
    "    l = list(np.array(val[f'label_mean']).reshape(-1))\n",
    "    tta_res = tta_res + s\n",
    "    label = label + l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0bdbe77d-1702-4dd6-975d-eb2d8ecdec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix(row):\n",
    "    return row[0]\n",
    "train['tta_score_fold_0'] = train['tta_score_fold_0'].apply(fix)\n",
    "train['tta_score_fold_1'] = train['tta_score_fold_1'].apply(fix)\n",
    "train['tta_score_fold_2'] = train['tta_score_fold_2'].apply(fix)\n",
    "train['tta_score_fold_3'] = train['tta_score_fold_3'].apply(fix)\n",
    "train.to_pickle(f\"./{EXP}/{MODEL}/tta.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5a15f08e-c8f5-4f06-a027-312d6a6f9ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8099878448924156"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.sqrt(mean_squared_error(label,tta_res))*4    \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ca41ea01-8798-4b42-bf34-a85f8d70f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = [pd.read_pickle(row).sort_values(\"id\").reset_index(drop=True) for row in glob.glob(\"./*/*/*tta*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b21fe273-d198-432b-a89a-c4c826cd419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_res = []\n",
    "label = []\n",
    "for fold in [0,1,2,3]:\n",
    "    val0 = train2[0][train2[0]['fold']==fold]\n",
    "    val1 = train2[1][train2[1]['fold']==fold]\n",
    "    val2 = train2[2][train2[2]['fold']==fold]\n",
    "    s = list(np.array(val0[f'tta_score_fold_{fold}']).reshape(-1) + np.array(val1[f'tta_score_fold_{fold}']).reshape(-1) + np.array(val2[f'tta_score_fold_{fold}']).reshape(-1))\n",
    "    s = np.array(s)/len(train2)\n",
    "    l = list(np.array(val0[f'label_mean']).reshape(-1))\n",
    "    tta_res = tta_res + list(s)\n",
    "    label = label + l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a9ee59d2-923d-402c-b05d-49c3610a56aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7754399632686458"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tta = np.sqrt(mean_squared_error(label,np.array(tta_res)))*4    \n",
    "res_tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2ebc7457-2ec6-4250-9d87-2a0ea9ac0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl = [pd.read_pickle(row).sort_values(\"id\").reset_index(drop=True) for row in glob.glob(\"*/*/*.pkl\") if \"_reg\" in row and \"tta\" not in row and \"large\" in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8111949b-2029-44cc-bf2d-e7c6b8ca4c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = [row['pred_label_mean'].values for row in pkl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "448e9b19-24d1-45e0-b5b5-fbe45c4c11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_res = []\n",
    "label = []\n",
    "for fold in [0,1,2,3]:\n",
    "    val0 = train2[0][train2[0]['fold']==fold]\n",
    "    val1 = train2[1][train2[1]['fold']==fold]\n",
    "    val2 = train2[2][train2[2]['fold']==fold]\n",
    "    s2 = [row[row['fold']==fold]['pred_label_mean'].values for row in pkl] + [row[row['fold']==fold]['pred_label_mean'].values for row in pkl]\n",
    "    s2.append(np.array(val0[f'tta_score_fold_{fold}']).reshape(-1))\n",
    "    s2.append(np.array(val1[f'tta_score_fold_{fold}']).reshape(-1))\n",
    "    s2.append(np.array(val2[f'tta_score_fold_{fold}']).reshape(-1))\n",
    "    s2 = np.mean(s2,axis=0)\n",
    "    l = list(np.array(val0[f'label_mean']).reshape(-1))\n",
    "    tta_res = tta_res + list(s2)\n",
    "    label = label + l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "47468832-2d72-4e11-9d4a-71074569267c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7632241102739699"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tta_ensemble = np.sqrt(mean_squared_error(label,np.array(tta_res)))*4    \n",
    "res_tta_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf79db7-f202-406d-96b4-792b48cfb70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mmdet)",
   "language": "python",
   "name": "mmdet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
